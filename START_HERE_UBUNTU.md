# ðŸš€ START HERE - Ubuntu PC Setup

**Welcome!** This guide will get your AIRR scraper running on Ubuntu in 5 minutes.

---

## ðŸŽ¯ What This Does

Automatically scrapes **2,968 products** from AIRR website daily:
- 11 warehouse locations per product
- ~30,000 total inventory records
- Saves to CSV + uploads to Supabase database
- Auto-cleans old data before each run
- Perfect for daily automation

---

## âš¡ Quick Start (5 Minutes)

### Step 1: Download Files (2 minutes)

**Easiest method:**
1. In Replit, click **â˜° menu** (top left)
2. Select **"Download as zip"**
3. On Ubuntu, extract:
   ```bash
   cd ~/Downloads
   unzip airr-scraper.zip -d ~/airr-scraper
   cd ~/airr-scraper
   ```

**Alternative:** See `UBUNTU_FILES_TO_DOWNLOAD.txt` for file list

### Step 2: Run Automated Setup (2 minutes)

```bash
cd ~/airr-scraper
./ubuntu_setup.sh
```

This installs:
- âœ… Python packages
- âœ… Playwright browser
- âœ… Creates .env config file

### Step 3: Add Your Credentials (1 minute)

```bash
nano .env
```

Fill in:
```bash
airr_USERNAME=your_actual_username
airr_PASSWORD=your_actual_password

SUPABASE_HOST=your_supabase_host
SUPABASE_DBNAME=postgres
SUPABASE_USER=postgres.yourprojectid
SUPABASE_PASSWORD=your_database_password
SUPABASE_PORT=6543
```

Save: `Ctrl+X`, `Y`, `Enter`

### Step 4: Test Everything Works

First, verify .env file is loaded correctly:
```bash
python3 test_env_loading.py
```

Should show: âœ… SUCCESS! All credentials loaded correctly from .env file

Then run full system test:
```bash
python3 test_setup.py
```

Should show: âœ… ALL TESTS PASSED!

**If .env test fails:**
- Check .env file exists: `ls -la .env`
- View .env content: `cat .env`
- Make sure no extra spaces or quotes around values
- Each line should be: `KEY=value` (no spaces around =)

### Step 5: Run the Scraper

```bash
python3 daily_scraper.py
```

**Done!** Wait 2-3 hours for completion.

---

## ðŸ“Š What Happens When You Run

```
ðŸ§¹ CLEANUP (automatic)
  â†’ Deletes old CSV files
  â†’ Deletes checkpoints
  â†’ Drops database table

ðŸ” AUTHENTICATION
  â†’ Logs into AIRR website
  â†’ Gets fresh cookies & token

ðŸ•·ï¸ SCRAPING
  â†’ Scrapes 2,968 products
  â†’ 11 warehouses each
  â†’ Saves to CSV

ðŸ“¤ DATABASE UPLOAD
  â†’ Creates fresh table
  â†’ Uploads ~30,000 rows
  â†’ Creates indexes

âœ… COMPLETE!
```

**Result:** Fresh inventory snapshot in CSV + database

---

## â° Daily Automation (Optional)

Run automatically every day at 5 PM:

```bash
crontab -e
```

Add:
```
0 17 * * * cd ~/airr-scraper && python3 daily_scraper.py >> scraper.log 2>&1
```

Check logs:
```bash
tail -f ~/airr-scraper/scraper.log
```

---

## ðŸ“– Documentation

**Start with these:**
- `README_UBUNTU.md` - Quick reference
- `UBUNTU_CHECKLIST.md` - Step-by-step checklist
- `UBUNTU_FILES_TO_DOWNLOAD.txt` - What to download

**Detailed guides:**
- `UBUNTU_SETUP.md` - Complete installation guide
- `DOWNLOAD_GUIDE.md` - Download methods
- `CLEAN_AND_RESTART.md` - How cleanup works
- `DATABASE_SETUP.md` - Database configuration

---

## ðŸ”§ Troubleshooting

### Setup fails?
```bash
# Try manual installation
sudo apt install python3 python3-pip -y
pip3 install -r requirements.txt
python3 -m playwright install chromium
sudo python3 -m playwright install-deps chromium
```

### Authentication fails?
```bash
# Test login
python3 login_and_save_cookies_.py

# Check credentials
cat .env
```

### Something broken?
```bash
# Run diagnostic
python3 test_setup.py

# Clean and restart
python3 clean_all_data.py
python3 daily_scraper.py
```

---

## ðŸ’¡ Pro Tips

1. **Test with small dataset first:**
   ```bash
   head -n 6 airr_sku_rows.csv > test.csv
   ```

2. **Run in background:**
   ```bash
   nohup python3 daily_scraper.py > scraper.log 2>&1 &
   ```

3. **Monitor progress:**
   ```bash
   tail -f scraper.log
   ```

4. **Stop if needed:**
   ```bash
   pkill -f daily_scraper
   ```

---

## ðŸ“Š Expected Output

After successful run:

**Files:**
- `airr_product_data.csv` (~30,000 rows)
- `cookies.json` (authentication)
- `scraper.log` (execution log)

**Database:**
- Table: `airr_product_availability`
- Rows: ~30,000
- Products: 2,968 unique

**Runtime:** 2-3 hours

---

## âœ… Checklist

- [ ] Downloaded files from Replit
- [ ] Ran `./ubuntu_setup.sh`
- [ ] Configured `.env` with credentials
- [ ] Ran `python3 test_setup.py` (all passed)
- [ ] Ran `python3 daily_scraper.py`
- [ ] Verified CSV output
- [ ] Verified database upload
- [ ] (Optional) Set up cron automation

---

## ðŸ†˜ Need Help?

**Quick commands:**
```bash
python3 test_setup.py      # Test installation
python3 daily_scraper.py   # Run scraper
tail -f scraper.log        # View logs
pkill -f daily_scraper     # Stop scraper
```

**Documentation:**
- All guides are in markdown (.md) files
- Read them with: `cat FILENAME.md`
- Or open in text editor

---

## ðŸŽ‰ You're Ready!

Your scraper is configured to:
- âœ… Run on Ubuntu PC
- âœ… Use `.env` file (not Replit Secrets)
- âœ… Auto-clean old data
- âœ… Upload to Supabase
- âœ… Work with daily cron automation

**Just run:**
```bash
python3 daily_scraper.py
```

**And you're done!** ðŸš€

---

**Questions? Check:** `README_UBUNTU.md` or `UBUNTU_SETUP.md`
